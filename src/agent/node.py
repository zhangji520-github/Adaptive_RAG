import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from langchain_core.prompts import ChatPromptTemplate
from llm_utils import llm
from src.agent.prompt import QUESTIONING_REWRITING_PROMPT, RAG_PROMPT_TEMPLATE
from utils.log_utils import log
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from src.agent.conditional import retrieval_grade
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.prompts import PromptTemplate


# *******************ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹*******************
# post_processing
def format_docs(docs):
    """
    æ ¼å¼åŒ–æ–‡æ¡£ï¼Œæ”¯æŒä¸åŒç±»å‹çš„æ–‡æ¡£æ ¼å¼
    """
    formatted_docs = []
    for doc in docs:
        if hasattr(doc, 'page_content'):
            # Documentå¯¹è±¡ï¼ˆæ¥è‡ªå‘é‡æ•°æ®åº“ï¼‰
            formatted_docs.append(doc.page_content)
        elif isinstance(doc, str):
            # å­—ç¬¦ä¸²æ ¼å¼ï¼ˆæ¥è‡ªç½‘ç»œæœç´¢ï¼‰
            formatted_docs.append(doc)
        elif isinstance(doc, dict):
            # å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–å†…å®¹
            content = doc.get('content') or doc.get('text') or doc.get('snippet') or str(doc)
            formatted_docs.append(content)
        else:
            # å…¶ä»–æ ¼å¼ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
            formatted_docs.append(str(doc))
    
    return "\n\n".join(formatted_docs)

def generate(state):
    """
    Generate answer

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    log.info("*****Start generate answer*****")
    question = state["question"]
    documents = state["documents"]
    
    # RAG generation - å°†å­—ç¬¦ä¸²æ¨¡æ¿è½¬æ¢ä¸ºChatPromptTemplate
    rag_prompt = ChatPromptTemplate.from_messages([
        ("human", RAG_PROMPT_TEMPLATE)
    ])
    rag_chain = rag_prompt | llm | StrOutputParser()
    generation = rag_chain.invoke({"context": format_docs(documents), "question": question})
    return {"documents": documents, "question": question, "generation": generation}

# *******************å¯¹é€šè¿‡å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆretrieve èŠ‚ç‚¹ï¼‰å¾—åˆ°çš„æ–‡æ¡£è¿›è¡Œç›¸å…³æ€§è¯„ä¼°ï¼Œè¿‡æ»¤æ‰ä¸ç”¨æˆ·é—®é¢˜æ— å…³çš„å†…å®¹*******************
def grade_documents(state):
    """
    Grade documents
    """
    log.info("*****Start grade documents*****")
    question = state["question"]
    documents = state["documents"]
    
    filter_doc = []
    for d in documents:
        score = retrieval_grade.invoke({"question": question, "documents": d})
        if score.binary_score == "yes":
            filter_doc.append(d)
        else:
            continue
    return {"documents": filter_doc, "question": question}

# æ ¹æ®è¿‡æ»¤æƒ…å†µé€‰æ‹©æ˜¯å¦ç”Ÿæˆ/å›é€€é‡å†™é—®é¢˜ è·¯ç”±å‡½æ•°
def decide_to_generate(state):
    """
    Decide to generate or rewrite question
    """
    log.info("*****Start decide to generate or rewrite question*****")
    filtered_documents = state["documents"]
    if not filtered_documents:
        print("ğŸ˜’Decision: ALL documents are irrelevant to the question. I think I should rewrite the question.")
        return "rewrite_question"
    else:
        print("ğŸ˜ŠDecision: Some documents are relevant to the question. I think I should generate the answer.")
        return "generate_answer"
    
# *******************é—®é¢˜é‡å†™èŠ‚ç‚¹ é€šè¿‡æç¤ºè¯åœ¨è½¬åŒ–æˆRunnableå¯¹è±¡*******************
rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", QUESTIONING_REWRITING_PROMPT),
        ("human", "Here is the initial question: \n\n {question}\n\n Formulate an improved question.")
    ]
)
question_rewriter = rewrite_prompt | llm | StrOutputParser()    
def transform_query(state):
    """
    Transform the query to produce a better question.
    """
    log.info("*****Start transform the query to produce a better question*****")
    question = state["question"]
    documents = state["documents"]

    better_question = question_rewriter.invoke({"question": question})
    return {"question": better_question, "documents": documents}

def direct_answer(state):
    """
    Generate direct answer for simple questions without retrieval
    """
    log.info("*****Start generate direct answer*****")
    question = state["question"]
    
    # ä¸ºç®€å•é—®é¢˜ç”Ÿæˆç›´æ¥å›ç­”
    direct_prompt = ChatPromptTemplate.from_messages([
        ("human", "è¯·å¯¹ä»¥ä¸‹é—®é¢˜ç»™å‡ºä¿çš®çš„å›ç­”ï¼š{question}")
    ])
    direct_chain = direct_prompt | llm | StrOutputParser()
    generation = direct_chain.invoke({"question": question})
    return {"question": question, "generation": generation, "documents": []}


if __name__ == "__main__":
    res = generate({"question": "åŠå¯¼ä½“ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ", "documents": [Document(page_content="åŠå¯¼ä½“ä¼˜åŠ¿æ˜¯èƒ½å¤Ÿæé«˜ç”Ÿäº§æ•ˆç‡å’Œäº§å“è´¨é‡ï¼Œé™ä½æˆæœ¬ï¼Œæé«˜ç«äº‰åŠ›ã€‚")]})
    print(res)