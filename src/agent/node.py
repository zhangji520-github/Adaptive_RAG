import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from langchain_core.prompts import ChatPromptTemplate
from llm_utils import llm
from src.agent.prompt import QUESTIONING_REWRITING_PROMPT, RAG_PROMPT_TEMPLATE
from utils.log_utils import log
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from src.agent.conditional import retrieval_grade
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.prompts import PromptTemplate







# *******************ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹*******************
# post_processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def generate(state):
    """
    Generate answer

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    log.info("*****Start generate answer*****")
    question = state["question"]
    documents = state["documents"]
    
    # RAG generation - å°†å­—ç¬¦ä¸²æ¨¡æ¿è½¬æ¢ä¸ºChatPromptTemplate
    rag_prompt = ChatPromptTemplate.from_messages([
        ("human", RAG_PROMPT_TEMPLATE)
    ])
    rag_chain = rag_prompt | llm | StrOutputParser()
    generation = rag_chain.invoke({"context": format_docs(documents), "question": question})
    return {"documents": documents, "question": question, "generation": generation}

# *******************å¯¹é€šè¿‡å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆretrieve èŠ‚ç‚¹ï¼‰å¾—åˆ°çš„æ–‡æ¡£è¿›è¡Œç›¸å…³æ€§è¯„ä¼°ï¼Œè¿‡æ»¤æ‰ä¸ç”¨æˆ·é—®é¢˜æ— å…³çš„å†…å®¹*******************
def grade_documents(state):
    """
    Grade documents
    """
    log.info("*****Start grade documents*****")
    question = state["question"]
    documents = state["documents"]
    
    filter_doc = []
    for d in documents:
        score = retrieval_grade.invoke({"question": question, "documents": d})
        if score.binary_score == "yes":
            filter_doc.append(d)
        else:
            continue
    return {"documents": filter_doc, "question": question}

# æ ¹æ®è¿‡æ»¤æƒ…å†µé€‰æ‹©æ˜¯å¦ç”Ÿæˆ/å›é€€é‡å†™é—®é¢˜ è·¯ç”±å‡½æ•°
def decide_to_generate(state):
    """
    Decide to generate or rewrite question
    """
    log.info("*****Start decide to generate or rewrite question*****")
    filtered_documents = state["documents"]
    if not filtered_documents:
        print("ğŸ˜’Decision: ALL documents are irrelevant to the question. I think I should rewrite the question.")
        return "rewrite_question"
    else:
        print("ğŸ˜ŠDecision: Some documents are relevant to the question. I think I should generate the answer.")
        return "generate_answer"
    
# *******************é—®é¢˜é‡å†™èŠ‚ç‚¹ é€šè¿‡æç¤ºè¯åœ¨è½¬åŒ–æˆRunnableå¯¹è±¡*******************
rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", QUESTIONING_REWRITING_PROMPT),
        ("human", "Here is the initial question: \n\n {question}\n\n Formulate an improved question.")
    ]
)
question_rewriter = rewrite_prompt | llm | StrOutputParser()    
def transform_query(state):
    """
    Transform the query to produce a better question.
    """
    log.info("*****Start transform the query to produce a better question*****")
    question = state["question"]
    documents = state["documents"]

    better_question = question_rewriter.invoke({"question": question})
    return {"question": better_question, "documents": documents}


if __name__ == "__main__":
    res = generate({"question": "åŠå¯¼ä½“ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ", "documents": [Document(page_content="åŠå¯¼ä½“ä¼˜åŠ¿æ˜¯èƒ½å¤Ÿæé«˜ç”Ÿäº§æ•ˆç‡å’Œäº§å“è´¨é‡ï¼Œé™ä½æˆæœ¬ï¼Œæé«˜ç«äº‰åŠ›ã€‚")]})
    print(res)