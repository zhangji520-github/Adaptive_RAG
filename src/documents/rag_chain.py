import sys
import os

from sympy import vector
# æ·»åŠ ä¸Šçº§ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from llm_utils import qwen_embeddings, openai_embedding, llm
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langchain.prompts import PromptTemplate
from markdown_parser import MarkdownParser
from langchain_milvus import Milvus, BM25BuiltInFunction
from env_utils import MILVUS_URI, COLLECTION_NAME
# Define the prompt template for generating AI responses
PROMPT_TEMPLATE = """
Human: You are an AI assistant, and you provide answers to questions by using fact based and statistical information when possible.
Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
<context>
{context}
</context>

<question>
{question}
</question>

The response should be specific and use statistics or numbers when possible.

Assistant:"""

# Create a PromptTemplate instance with the defined template and input variables
prompt = PromptTemplate(
    template=PROMPT_TEMPLATE, input_variables=["context", "question"]
)

class RagChain:
    """è‡ªå®šä¹‰ragchain"""
    @staticmethod
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    def run_chain(self, retrieval, question):
        # é¦–å…ˆèŽ·å–æ£€ç´¢ç»“æžœ
        retrieved_docs = retrieval.invoke(question)
        
        # æ‰“å°æ£€ç´¢ç»“æžœ
        print("ðŸ” æ£€ç´¢ç»“æžœ:")
        print("=" * 40)
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"æ–‡æ¡£ç‰‡æ®µ {i}:")
            print(f"å†…å®¹: {doc.page_content}")
            print("-" * 30)
        
        # æž„å»ºRAGé“¾
        rag_chain = (
            {"context": retrieval | RagChain.format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # ç”Ÿæˆç­”æ¡ˆ
        print("\nðŸ¤– ç”Ÿæˆç­”æ¡ˆ:")
        # res = rag_chain.invoke(question)
        # print(res)
        for chunk in rag_chain.stream(question):
            print(chunk, end="", flush=True)

def create_milvus_connection():
    """è¿žæŽ¥åˆ°å·²ç»åˆ›å»ºå¥½çš„ Milvus å‘é‡æ•°æ®åº“"""
    vectorstore = Milvus(
        embedding_function=openai_embedding,
        collection_name=COLLECTION_NAME,
        connection_args={
            "uri": MILVUS_URI,
        },
        # è‡ªåŠ¨å°†æ–‡æœ¬å­—æ®µï¼ˆTEXT_FIELDï¼‰é€šè¿‡ å†…ç½®çš„ BM25 å‡½æ•° è½¬æ¢ä¸º ç¨€ç–å‘é‡ï¼ˆSPARSE_FLOAT_VECTORï¼‰
        builtin_function=BM25BuiltInFunction( 
            input_field_names="text",      # è¾“å…¥ï¼šåŽŸå§‹æ–‡æœ¬å­—æ®µ 	VARCHAR
            output_field_names="sparse",   # è¾“å‡ºï¼šç¨€ç–å‘é‡å­—æ®µï¼Œå¯¹åº” vector_field[0] SPARSE_FLOAT_VECTOR
        ), 
        vector_field=["sparse", "dense"],  # æŒ‡å®šè¦å­˜å‚¨çš„å‘é‡å­—æ®µ dense ç”¨äºŽ OpenAI åµŒå…¥ï¼Œ sparse ç”¨äºŽ BM25 å‡½æ•°ã€‚
        consistency_level="Strong",          # ä¸€è‡´æ€§çº§åˆ«
        drop_old=False,                       # æ˜¯å¦åˆ é™¤å·²æœ‰çš„ collection
    )
    return vectorstore


if __name__ == "__main__":
    # # 1. åŠ è½½æ–‡æ¡£æ•°æ®
    # file_path = r"E:\Workspace\ai\RAG\datas\md\tech_report_z7tx05vt.md"
    # parser = MarkdownParser()
    # docs = parser.parse_markdown_to_documents(file_path)
    # print(f"åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£å—")

    # 2. åˆ›å»º Milvus å‘é‡æ•°æ®åº“å¹¶æ·»åŠ æ–‡æ¡£ 
    vectorstore = create_milvus_connection()

    # 3. åˆ›å»º RAG é“¾å¹¶æµ‹è¯•
    rag = RagChain()
    
    
    # æ–¹æ³•2ï¼šå¦‚æžœéœ€è¦æ··åˆæ£€ç´¢å¹¶æ·»åŠ è¿‡æ»¤æ¡ä»¶ï¼Œå¯ä»¥è¿™æ ·è®¾ç½®ï¼š
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": 5,
            "ranker_type": "weighted",          # ä¹Ÿå¯ä»¥ä½¿ç”¨ "rrf" è¿™æ ·çš„è¯paramsè®¾ç½® {"k":100}
            "ranker_params": {"weights": [0.3, 0.7]},  # sparseæƒé‡0.3ï¼Œdenseæƒé‡0.7
            "expr": 'category == "TitleWithContent" && category_depth > 1'
        }
    )
    

    
    # 5. æµ‹è¯•é—®é¢˜
    test_questions = [
        "åŠå¯¼ä½“åˆ¶é€ ä¸­çš„è™šæ‹Ÿè®¡é‡æŠ€æœ¯ï¼Ÿ",
    ]
    
    print("\nå¼€å§‹æµ‹è¯•æ··åˆæ£€ç´¢:")
    # ç§»é™¤ output_fields å‚æ•°ä»¥é¿å…ä¸Žæ··åˆæœç´¢çš„å‚æ•°å†²çª  å¦‚æžœæƒ³è¦æ›´åŠ ç²¾ç»†çš„æŽ§åˆ¶ å¯ä»¥ç”¨ å®¢æˆ·ç«¯milvusè‡ªå·±æä¾›çš„åŽŸç”Ÿ hybrid_search æ–¹æ³• è‡ªå·±å†™ ANNSearchRequest
    results = vectorstore.similarity_search_with_score(
        "å¹²æ³•åˆ»èš€çš„ä¼˜åŠ¿", 
        k=4, 
        ranker_type="weighted", 
        ranker_params={"weights": [0.3, 0.7]},
        expr='category == "TitleWithContent" && category_depth > 1'
    )
    print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç»“æžœ:")
    for i, (doc, score) in enumerate(results, 1):
        print(f"ç»“æžœ {i}:")
        print(f"å†…å®¹: {doc.page_content}")
        print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {score}")
        print("-" * 30)

    # print("\nå¼€å§‹æµ‹è¯• RAG ç³»ç»Ÿ:")
    # print("=" * 50)
    
    # for i, question in enumerate(test_questions, 1):
    #     print(f"\né—®é¢˜ {i}: {question}")
    #     print("-" * 30)
        
    #     try:
    #         rag.run_chain(retriever, question)
    #     except Exception as e:
    #         print(f"é”™è¯¯: {e}")
        
    #     print("-" * 50)
